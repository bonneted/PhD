# Physics-Informed Neural Networks: A PhD Thesis

A comprehensive exploration of Physics-Informed Neural Networks (PINNs) across multiple research chapters, from foundational implementations to advanced applications in inverse problems and uncertainty quantification.

---

## Project Structure

```
PhD/
├── README.md                          # This file
├── _DetailedPlan.md                   # Thesis outline and chapter structure
│
├── Notebooks/                         # Core research artifacts (produce all results & figures)
│   ├── II_PINNs.ipynb                # Ch. II: PINN fundamentals with JAX & DeepXDE
│   ├── III_SeparablePINNs.ipynb      # Ch. III: Separable architectures for scalability
│   ├── IV_ImprovingTraining.ipynb    # Ch. IV: Training optimization techniques
│   ├── V_InverseProblems.ipynb       # Ch. V: Material parameter identification
│   └── VI_UncertaintyPropagation.ipynb # Ch. VI: Uncertainty quantification
│
├── examples/                          # Reusable implementations (Config-driven via Hydra pattern)
│   └── allen_cahn/
│       ├── allen_cahn_model.py       # Core model (DEFAULT_CONFIG + pde_allen_cahn + train_allen_cahn)
│       ├── manual_run.ipynb          # Standalone execution example
│       └── dataset/
│           └── Allen_Cahn.mat        # Training/evaluation data
│
├── results/                           # Output directory (auto-generated by notebooks)
│   ├── II_PINNs/
│   │   └── images/
│   │       ├── pgf/                  # LaTeX-compatible PGF figures
│   │       │   ├── activation_functions.pgf
│   │       │   ├── jax_computational_graph.pgf
│   │       │   ├── jax_jit_performance.pgf
│   │       │   ├── jaxpr_computation_graph.pgf
│   │       │   └── pinn_poisson_1d.pgf
│   │       └── *.png                 # PNG exports for quick preview
│   │
│   ├── III_SeparablePINNs/
│   │   └── [figures, metrics]
│   │
│   ├── IV_ImprovingTraining/
│   │   ├── HyperparameterSearch/
│   │   │   ├── AllRuns_Allen-Cahn.csv  # Aggregated sweep results
│   │   │   └── damien-bonnet/          # Per-run artifacts (tracked via W&B)
│   │   │       └── Allen-Cahn/
│   │   │           └── {run_id}/
│   │   │               ├── checkpoint
│   │   │               └── metrics.json
│   │   ├── IV_2_FourierFeatures_Figures/
│   │   ├── IV_2_FourierFeatures_Tables/
│   │   ├── IV_2_SelfAttention_Tables/
│   │   └── [comparison plots, loss curves]
│   │
│   ├── V_InverseProblems/
│   │   └── [reconstruction results, validation plots]
│   │
│   └── VI_UncertaintyPropagation/
│       └── [UQ metrics, ensemble results, PCE outputs]
│
├── .vscode/
│   ├── launch.json                   # Debugger config (library inspection enabled)
│   └── settings.json                 # Workspace settings (Jupyter debugging)
│
└── .gitignore                         # Ignore results/, .cache/, *.mat
```

---

## Chapter Breakdown

| Chapter | Notebook | Focus | Key Results |
|---------|----------|-------|-------------|
| **II** | `II_PINNs.ipynb` | PINN fundamentals, autodiff, JAX/DeepXDE implementations | Poisson equation solver, activation functions, JIT performance |
| **III** | `III_SeparablePINNs.ipynb` | Low-rank separable architectures | Computational cost reduction, Allen-Cahn experiments |
| **IV** | `IV_ImprovingTraining.ipynb` | Learning rates, regularization, Fourier features, attention | Hyperparameter sweep (W&B), training curves, convergence analysis |
| **V** | `V_InverseProblems.ipynb` | Material parameter identification, inverse PINNs | Stress/strain reconstruction, robustness to noise, geometry mapping |
| **VI** | `VI_UncertaintyPropagation.ipynb` | Bayesian PINNs, ensemble methods, polynomial chaos | Epistemic/aleatoric UQ, Poisson + composite plate applications |

---

## Configuration Management (Hydra Pattern)

### Overview

Instead of hard-coding hyperparameters, we adopt a **config-first approach** inspired by [Hydra](https://hydra.cc/):

- **`DEFAULT_CONFIG`**: Base hyperparameters (in `examples/allen_cahn/allen_cahn_model.py`)
- **Config Override**: Runtime parameter updates via function arguments
- **Sweep Tracking**: Integration with Weights & Biases (W&B) for hyperparameter search

### Example: Allen-Cahn Model

#### Default Configuration

```python
# examples/allen_cahn/allen_cahn_model.py

DEFAULT_CONFIG = {
    "fourier_features": True,
    "n_fourier_features": 128,
    "sigma": 10,
    "net_type": "SPINN",           # PINN or SPINN
    "mlp_type": "mlp",             # modified-mlp, mlp, etc.
    "activations": "sin",          # sin, tanh, etc.
    "initialization": "Glorot normal",
    "n_hidden": 3,
    "rank": 64,
    "num_domain": 22500,           # 150² collocation points
    "lr": [1e-3, 1e-4, 5e-5],      # Learning rate schedule
    "lr_decay": None,
    "n_iter": 30000,
    "seed": 0,
    "pde_coefficient": 0.001,      # d parameter
    "SA": False,                   # Self-Attention
    "SA_init": "constant",
    "SA_update_factor": -1.0,
}

class Config:
    def __init__(self, **entries):
        self.__dict__.update(entries)
```

#### Training Interface

```python
from examples.allen_cahn.allen_cahn_model import train_allen_cahn, DEFAULT_CONFIG

# Standalone: use defaults
results = train_allen_cahn()

# Override specific parameters
custom_config = {
    "net_type": "PINN",
    "n_iter": 5000,
    "lr": 1e-3,
}
results = train_allen_cahn(config=custom_config)

# Returns
results = {
    "config": {...},                          # Full config with defaults
    "model": model,                           # Trained DeepXDE model
    "losshistory": losshistory,              # Training curves
    "evaluation": {                           # Evaluation metrics
        "u_pred": u_pred,
        "l2_error": 0.0012,
        "mean_pde_residual": 1.5e-4,
        ...
    },
    "elapsed_time": 125.3,
    "iterations_per_sec": 239.5,
}
```

### Hyperparameter Search with Weights & Biases

#### W&B Sweep Configuration

```yaml
# sweep_config.yaml
program: examples/allen_cahn/allen_cahn_model.py
method: bayes
metric:
  name: l2_relative_error
  goal: minimize
parameters:
  n_fourier_features:
    values: [64, 128, 256]
  sigma:
    values: [5, 10, 20]
  n_hidden:
    values: [2, 3, 4]
  lr:
    values: [[1e-3, 1e-4], [1e-3, 1e-4, 5e-5]]
  SA:
    values: [true, false]
```

#### Launch Sweep

```bash
# Initialize W&B project
wandb login

# Create and run sweep
sweep_id=$(wandb sweep sweep_config.yaml --project pinn-sweeps)
wandb agent $sweep_id --count 50

# Results stored in:
# results/IV_ImprovingTraining/HyperparameterSearch/
```

#### Results Aggregation

After sweep completion, aggregate results:

```python
# In IV_ImprovingTraining.ipynb

import pandas as pd
import wandb

# Download sweep results from W&B
api = wandb.Api()
runs = api.sweep("username/pinn-sweeps/sweep_id").runs
df = pd.DataFrame([run.summary for run in runs])

# Save aggregated results
df.to_csv("results/IV_ImprovingTraining/HyperparameterSearch/AllRuns_Allen-Cahn.csv")

# Group by configuration (removing duplicates, normalizing lr_decay)
grouped = df.groupby(
    ["net_type", "n_fourier_features", "sigma", "activations"],
    as_index=False
).agg({
    "l2_relative_error": "mean",
    "mean_pde_residual": "mean",
    "elapsed_time_s": "mean",
})
```

---

## Workflow: From Notebook to LaTeX

### 1. Run Notebook → Generate Results

```python
# II_PINNs.ipynb

import os
results_folder = 'results/II_PINNs'
os.makedirs(os.path.join(results_folder, 'images', 'pgf'), exist_ok=True)

# Plot and save
fig.savefig(os.path.join(results_folder, 'images', 'pinn_poisson_1d.pgf'))
fig.savefig(os.path.join(results_folder, 'images', 'pinn_poisson_1d.png'))
```

### 2. Include in LaTeX

```latex
\documentclass{article}
\usepackage{pgf}

\begin{document}

\chapter{Physics-Informed Neural Networks}

\begin{figure}[h]
  \centering
  \input{../results/II_PINNs/images/pgf/pinn_poisson_1d.pgf}
  \caption{PINN training evolution on 1D Poisson equation.}
\end{figure}

\input{../results/IV_ImprovingTraining/IV_2_FourierFeatures_Tables/one_run.tex}

\end{document}
```

### 3. Tracking

```
Git repository:
├── .gitignore → Ignore results/ and large datasets
├── Tracked: Notebooks, code, configs
├── NOT tracked: results/, *.mat, W&B artifacts
```

---

## Technologies & Stack

| Layer | Technologies | Purpose |
|-------|--------------|---------|
| **Deep Learning** | JAX, Flax, DeepXDE | Core PDE solvers and neural networks |
| **Optimization** | Optax, Adam | Training algorithms |
| **Data I/O** | NumPy, SciPy, Pandas | Data handling and I/O |
| **Visualization** | Matplotlib, scienceplots, Plotly | Figures and interactive plots |
| **Config Management** | Hydra pattern (manual + W&B) | Hyperparameter tracking |
| **Experiment Tracking** | Weights & Biases (W&B) | Sweep results, logging |
| **Documentation** | Jupyter, PGF/LaTeX | Research narratives and publication figures |
| **Development** | VS Code, Jupyter, debugpy | Interactive development and debugging |

---

## Installation & Setup

### 1. Clone and Navigate

```bash
cd ~/PhD
```

### 2. Environment Setup

```bash
# Create conda environment with JAX (CPU or GPU)
conda create -n jax python=3.11

# Activate
conda activate jax

# Install core dependencies
pip install --upgrade "jax[cpu]>=0.8.0" "flax>=0.11.2,<0.12" "optax>=0.2.0" \
    deepxde numpy scipy pandas matplotlib scienceplots plotly \
    jupyter jupyterlab ipykernel wandb

# (Optional) GPU: replace [cpu] with [cuda12]
```

### 3. Jupyter Kernel

```bash
python -m ipykernel install --user --name jax --display-name "JAX (GPU)"
```

### 4. W&B Setup

```bash
wandb login
# Follow browser prompt to authenticate
```

---

## Running Examples

### Standalone Allen-Cahn Training

```bash
cd ~/PhD
python examples/allen_cahn/allen_cahn_model.py
```

### Notebook Execution

```bash
jupyter lab II_PINNs.ipynb
# Run all cells (Ctrl+Shift+Enter in Jupyter)
```

### Hyperparameter Sweep

```bash
# Configure sweep in sweep_config.yaml, then:
wandb sweep sweep_config.yaml --project pinn-research
wandb agent <SWEEP_ID> --count 100
```

---

## Key Features

✅ **Modular Design**: Examples are standalone Python modules reusable in notebooks  
✅ **Config-Driven**: Hydra-style parameter management for reproducibility  
✅ **Sweep Integration**: W&B integration for systematic hyperparameter search  
✅ **LaTeX Export**: PGF figures for seamless thesis integration  
✅ **Debugging**: VS Code integration with library inspection enabled  
✅ **Reproducibility**: Fixed seeds, tracked configs, version control  

---

## Troubleshooting

### Import Errors

```bash
# Ensure kernel uses correct environment
python -m ipykernel list  # Check registered kernels
conda activate jax
jupyter notebook --no-browser --ip=127.0.0.1
```

### CUDA Issues

```bash
# Check JAX device detection
python -c "import jax; print(jax.devices())"

# Fall back to CPU if needed
export JAX_PLATFORMS=cpu
```

### W&B Offline Mode

```python
import os
os.environ["WANDB_MODE"] = "offline"
# Sync later: wandb sync <offline_run_path>
```

---

## Publications & References

- **Raissi et al. (2019)**: Physics-Informed Neural Networks (PINNs) foundational work
- **Perdikaris et al. (2017)**: Probabilistic PINNs and uncertainty quantification
- **Lu et al. (2021)**: Fourier feature embeddings for improved PINNs
- **Weights & Biases**: Sweep documentation at https://docs.wandb.ai/

---

## Author

Damien Bonnet  
PhD Candidate  
[Institution]

---

## License

This project is provided for educational and research purposes.
