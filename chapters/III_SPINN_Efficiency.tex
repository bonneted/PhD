% III_SPINN_Efficiency.tex
% Detailed explanation of SPINN architecture and computational efficiency

\section{Separable Physics-Informed Neural Networks (SPINN)}
\label{sec:spinn}

\subsection{Architecture}
\label{sec:spinn_architecture}

The Separable Physics-Informed Neural Network (SPINN) exploits the structure of PDE problems defined on tensor-product domains to dramatically reduce computational cost. The key insight is to decompose the solution approximation into a sum of separable rank-1 tensors, each formed by products of 1D feature functions.

\subsubsection{Body Networks}

SPINN consists of $d$ \emph{body-networks}, where $d$ is the spatial dimension. Each body-network $f^{(\theta_i)}: \mathbb{R} \to \mathbb{R}^r$ is a multi-layer perceptron (MLP) that takes a single coordinate component $x_i$ as input and outputs an $r$-dimensional feature vector:
\begin{equation}
    f^{(\theta_i)}(x_i) = \begin{pmatrix} f^{(\theta_i)}_1(x_i) \\ f^{(\theta_i)}_2(x_i) \\ \vdots \\ f^{(\theta_i)}_r(x_i) \end{pmatrix} \in \mathbb{R}^r,
    \label{eq:body_network}
\end{equation}
where $\theta_i$ denotes the parameters of the $i$-th body-network and $r$ is the \emph{rank} of the decomposition.

\subsubsection{Feature Merging}

The final prediction is computed by combining the feature vectors from all body-networks through a simple product-and-sum operation:
\begin{equation}
    \hat{u}(x_1, x_2, \ldots, x_d) = \sum_{j=1}^{r} \prod_{i=1}^{d} f^{(\theta_i)}_j(x_i).
    \label{eq:spinn_output}
\end{equation}

This formulation corresponds to a CP (Canonical Polyadic) tensor decomposition, where the solution tensor is approximated as a sum of $r$ rank-1 tensors.

\subsubsection{Batchified Formulation}

In practice, we sample $N$ coordinates from each axis, forming a structured grid of $N^d$ collocation points. Let $\mathbf{x}_i \in \mathbb{R}^N$ denote the vector of $N$ sampled coordinates along axis $i$. The feature tensor $\mathbf{F} \in \mathbb{R}^{N \times r \times d}$ is computed by applying each body-network to its respective coordinate vector:
\begin{equation}
    \mathbf{F}_{:,:,i} = f^{(\theta_i)}(\mathbf{x}_i) \in \mathbb{R}^{N \times r}.
    \label{eq:feature_tensor}
\end{equation}

The solution tensor $\hat{\mathbf{U}} \in \mathbb{R}^{N \times N \times \cdots \times N}$ (with $d$ dimensions) is then reconstructed via:
\begin{equation}
    \hat{\mathbf{U}} = \sum_{j=1}^{r} \bigotimes_{i=1}^{d} \mathbf{F}_{:,j,i},
    \label{eq:spinn_batch}
\end{equation}
where $\bigotimes$ denotes the outer product.

\subsubsection{Extension to Vector-Valued Outputs}

For problems with $m$ output components (e.g., displacement fields in 2D/3D), SPINN uses an expanded feature dimension $r \cdot m$. Each body-network outputs $\mathbf{F} \in \mathbb{R}^{N \times r \times m}$, and the $k$-th output component is:
\begin{equation}
    \hat{u}_k(\mathbf{x}) = \sum_{j=1}^{r} \prod_{i=1}^{d} f^{(\theta_i)}_{j,k}(x_i), \quad k = 1, \ldots, m.
    \label{eq:spinn_vector}
\end{equation}

%==============================================================================
\subsection{Computational Efficiency: Forward Pass}
\label{sec:spinn_forward_efficiency}

We now analyze the computational cost of evaluating the network on a grid of $N^d$ points.

\subsubsection{PFNN (Standard PINN) Complexity}

A standard Parallel Fourier Neural Network (PFNN) or conventional PINN processes each of the $N^d$ points independently through the full network. For an MLP with $L$ layers and width $W$, the number of floating-point operations per point is approximately:
\begin{equation}
    \text{ops}_{\text{MLP}} \approx 2W \cdot d_{\text{in}} + \sum_{\ell=1}^{L-1} 2W^2 + 2W \cdot d_{\text{out}}.
\end{equation}

The total cost for PFNN scales as:
\begin{equation}
    C_{\text{PFNN}} = N^d \cdot \text{ops}(f) = \mathcal{O}(N^d).
    \label{eq:pfnn_cost}
\end{equation}

\subsubsection{SPINN Complexity}

SPINN separates the computation into two phases:

\paragraph{Phase 1: Body-network evaluation.} 
Each of the $d$ body-networks processes only $N$ points (one axis at a time):
\begin{equation}
    C_{\text{body}} = N \cdot d \cdot \text{ops}(f) = \mathcal{O}(Nd).
    \label{eq:spinn_body_cost}
\end{equation}

\paragraph{Phase 2: Feature merging.}
The outer product and summation in Eq.~\eqref{eq:spinn_batch} operates on the $N^d$ output tensor, but involves only simple element-wise multiplications and additions:
\begin{equation}
    C_{\text{merge}} = N^d \cdot r \cdot d \cdot c_h = \mathcal{O}(N^d \cdot r),
    \label{eq:spinn_merge_cost}
\end{equation}
where $c_h$ is a small constant (typically $c_h \ll \text{ops}(f)$ since no nonlinear activations are involved).

\paragraph{Total SPINN cost:}
\begin{equation}
    C_{\text{SPINN}} = N \cdot d \cdot \text{ops}(f) + N^d \cdot r \cdot d \cdot c_h.
    \label{eq:spinn_total_cost}
\end{equation}

\subsubsection{Speedup Analysis}

The ratio of costs is:
\begin{equation}
    \frac{C_{\text{SPINN}}}{C_{\text{PFNN}}} = \frac{N \cdot d \cdot \text{ops}(f) + N^d \cdot r \cdot d \cdot c_h}{N^d \cdot \text{ops}(f)} = \frac{d}{N^{d-1}} + \frac{r \cdot d \cdot c_h}{\text{ops}(f)}.
    \label{eq:speedup_ratio}
\end{equation}

As $N$ increases, the first term vanishes ($\propto N^{-(d-1)}$), and the ratio approaches:
\begin{equation}
    \frac{C_{\text{SPINN}}}{C_{\text{PFNN}}} \xrightarrow{N \to \infty} \frac{r \cdot d \cdot c_h}{\text{ops}(f)} \ll 1.
\end{equation}

However, for finite $N$, the merging cost $N^d \cdot r$ is not fully negligible. Table~\ref{tab:efficiency_summary} and Figure~\ref{fig:spinn_efficiency} illustrate this behavior empirically. At $N=256$ (65,536 total points), SPINN achieves approximately 89$\times$ fewer FLOPs than PFNN for the forward pass.

%==============================================================================
\subsection{Computational Efficiency: Gradient Computation}
\label{sec:spinn_grad_efficiency}

Computing derivatives is essential for training PINNs, as the PDE residual involves $\partial \hat{u}/\partial x_i$. This is where SPINN achieves its most significant advantage.

\subsubsection{Jacobian Structure}

For a function $\hat{u}: \mathbb{R}^d \to \mathbb{R}^m$ evaluated at $N^d$ points, the full Jacobian has shape $(N^d \times m \times d)$. We analyze the cost of computing the spatial derivatives $\partial \hat{u}/\partial x_i$ for all outputs at all points.

\subsubsection{PFNN Gradient Cost}

\paragraph{Reverse-mode (backpropagation):}
Reverse-mode AD computes the gradient of a scalar with respect to all inputs in one pass. For a Jacobian with $m$ outputs, we need $m$ backward passes:
\begin{equation}
    C_{\text{PFNN}}^{\text{rev}} = m \cdot c_f \cdot N^d \cdot \text{ops}(f),
    \label{eq:pfnn_grad_reverse}
\end{equation}
where $c_f \in [2, 3]$ accounts for the additional cost of reverse-mode AD.

\paragraph{Forward-mode (JVP):}
Forward-mode AD computes directional derivatives with respect to one input direction per pass. For $d$ input dimensions:
\begin{equation}
    C_{\text{PFNN}}^{\text{fwd}} = d \cdot c_f \cdot N^d \cdot \text{ops}(f).
    \label{eq:pfnn_grad_forward}
\end{equation}

For PINNs, where typically $d < m$ (e.g., 2D problems with 5 stress/displacement outputs), forward-mode is more efficient.

\subsubsection{SPINN Gradient Cost}

The derivative of SPINN with respect to $x_i$ takes a particularly elegant form. From Eq.~\eqref{eq:spinn_output}:
\begin{equation}
    \frac{\partial \hat{u}}{\partial x_i} = \sum_{j=1}^{r} \left( \prod_{k \neq i} f^{(\theta_k)}_j(x_k) \right) \cdot \frac{\partial f^{(\theta_i)}_j(x_i)}{\partial x_i}.
    \label{eq:spinn_derivative}
\end{equation}

\paragraph{Key insight:} The derivative $\partial f^{(\theta_i)}(x_i)/\partial x_i$ only involves the $i$-th body-network, which processes only $N$ points (not $N^d$).

\paragraph{Forward-mode for SPINN:}
Using a single JVP pass, we obtain all $r$ components of the derivative $\partial f^{(\theta_i)}(x_i)/\partial x_i \in \mathbb{R}^r$ for $N$ input points:
\begin{equation}
    C_{\text{SPINN, body}}^{\text{fwd}} = N \cdot c_f \cdot \text{ops}(f).
    \label{eq:spinn_grad_body}
\end{equation}

For all $d$ input dimensions:
\begin{equation}
    C_{\text{SPINN}}^{\text{fwd}} = N \cdot d \cdot c_f \cdot \text{ops}(f) + C_{\text{merge}}.
    \label{eq:spinn_grad_total}
\end{equation}

\paragraph{Speedup for gradient computation:}
\begin{equation}
    \frac{C_{\text{SPINN}}^{\text{fwd}}}{C_{\text{PFNN}}^{\text{fwd}}} \approx \frac{N \cdot d}{d \cdot N^d} = \frac{1}{N^{d-1}}.
    \label{eq:grad_speedup}
\end{equation}

For $d = 2$ and $N = 256$, this yields a theoretical $256\times$ speedup for the body-network gradient computation alone! In practice, the observed speedup is approximately $6-7\times$ due to merging overhead and JIT compilation effects at smaller batch sizes.

\subsubsection{Reverse-Mode for SPINN}

If using reverse-mode AD on SPINN, we must backpropagate through all $d$ body-networks for each output:
\begin{equation}
    C_{\text{SPINN}}^{\text{rev}} = m \cdot d \cdot N \cdot c_f \cdot \text{ops}(f) + C_{\text{merge}}.
    \label{eq:spinn_grad_reverse}
\end{equation}

The ratio between reverse and forward mode for SPINN is:
\begin{equation}
    \frac{C_{\text{SPINN}}^{\text{rev}}}{C_{\text{SPINN}}^{\text{fwd}}} \approx \frac{m \cdot d}{d} = m,
\end{equation}
when the merging cost is negligible. However, due to the structure of automatic differentiation in practice, the ratio is closer to $d$ (the number of body-networks), as reverse-mode must trace gradients through all body-networks simultaneously.

%==============================================================================
\subsection{Summary of Complexity}
\label{sec:complexity_summary}

Table~\ref{tab:complexity} summarizes the computational complexity for different operations.

\begin{table}[htbp]
\centering
\caption{Computational complexity comparison between PFNN and SPINN for evaluating $N^d$ grid points with $m$ outputs. Here $f$ denotes the MLP operations and $h$ the (cheap) merging operations.}
\label{tab:complexity}
\begin{tabular}{lcc}
\toprule
\textbf{Operation} & \textbf{PFNN} & \textbf{SPINN} \\
\midrule
Forward pass & $\mathcal{O}(N^d \cdot \text{ops}(f))$ & $\mathcal{O}(Nd \cdot \text{ops}(f) + N^d \cdot \text{ops}(h))$ \\
Jacobian (forward-mode) & $\mathcal{O}(d \cdot N^d \cdot \text{ops}(f))$ & $\mathcal{O}(Nd \cdot \text{ops}(f) + N^d \cdot \text{ops}(h))$ \\
Jacobian (reverse-mode) & $\mathcal{O}(m \cdot N^d \cdot \text{ops}(f))$ & $\mathcal{O}(md \cdot N \cdot \text{ops}(f) + N^d \cdot \text{ops}(h))$ \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[htbp]
\centering
\caption{Empirical speedup of SPINN over PFNN for different grid resolutions ($d=2$, $m=2$). Forward pass speedup measured via FLOPs; gradient speedup measured via execution time.}
\label{tab:efficiency_summary}
\begin{tabular}{cccc}
\toprule
$N$ & Points ($N^2$) & Forward Speedup & Gradient Speedup \\
\midrule
16 & 256 & 14.8$\times$ & 0.9$\times$ \\
32 & 1,024 & 27.0$\times$ & 1.0$\times$ \\
64 & 4,096 & 44.4$\times$ & 1.0$\times$ \\
128 & 16,384 & 66.7$\times$ & 1.1$\times$ \\
256 & 65,536 & 89.1$\times$ & 6.2$\times$ \\
\bottomrule
\end{tabular}
\end{table}

The key advantages of SPINN are:
\begin{enumerate}
    \item \textbf{Forward pass}: Body-network evaluation scales as $\mathcal{O}(Nd)$ instead of $\mathcal{O}(N^d)$.
    \item \textbf{Gradient computation}: Each derivative $\partial/\partial x_i$ only requires differentiating the $i$-th body-network with $N$ inputs, not the full $N^d$ grid.
    \item \textbf{Forward-mode synergy}: Since body-networks have 1 input and $r$ outputs, forward-mode AD is optimal (one pass per input dimension).
\end{enumerate}

